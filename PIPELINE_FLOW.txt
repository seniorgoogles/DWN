===============================================================================
              JSC_COMPLEX.PY COMPLETE PIPELINE FLOW
===============================================================================

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 1: TRAIN DENSE MODEL                                             │
│ Duration: ~5 min                                                        │
├─────────────────────────────────────────────────────────────────────────┤
│ Input:  Dataset (JSC classification)                                   │
│ Model:  Linear(3200 → 10) → ReLU → Linear(10 → 5)                     │
│ Output: Dense model with ~75% accuracy                                 │
└─────────────────────────────────────────────────────────────────────────┘
                              ↓

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 2: ADAPTIVE PRUNING TO 6 INPUTS                                  │
│ Duration: ~20-30 min (configurable)                                    │
│ Code: Lines 1011-1031                                                   │
├─────────────────────────────────────────────────────────────────────────┤
│ Configuration:                                                          │
│   • final_n_weights = 6        ← Target: 6 inputs per neuron          │
│   • pruning_steps = 25         ← How many iterations                  │
│   • epochs_per_step = 6        ← Training after each prune            │
│   • pruning_steepness = 2.0    ← Early aggressiveness (NEW!)          │
│   • use_backtracking = True    ← Auto-adjust if accuracy drops        │
│                                                                         │
│ Process:                                                                │
│   Step 1:  64 weights/neuron → 52 weights → Retrain (6 epochs)        │
│   Step 2:  52 weights/neuron → 43 weights → Retrain (6 epochs)        │
│   Step 3:  43 weights/neuron → 35 weights → Retrain (6 epochs)        │
│   ...                                                                   │
│   Step 25: 7 weights/neuron → 6 weights → Retrain (18 epochs)         │
│                                                                         │
│ With Backtracking:                                                      │
│   If accuracy drops >5% → Restore checkpoint, halve step size, retry   │
│                                                                         │
│ Output: Pruned model (~68-70% accuracy)                                │
└─────────────────────────────────────────────────────────────────────────┘
                              ↓

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 3: FINE-TUNING (3 Phases)                                        │
│ Duration: ~10-15 min                                                    │
│ Code: Lines 1036-1088                                                   │
├─────────────────────────────────────────────────────────────────────────┤
│ Phase 1: Adam LR=5e-4, 30 epochs  → Gentle refinement                 │
│ Phase 2: Adam LR=1e-4, 20 epochs  → Precise tuning                    │
│ Phase 3: SGD momentum, 15 epochs  → Escape local minima               │
│                                                                         │
│ Output: Fine-tuned model (~71-73% accuracy)                            │
│         Improvement: +2-5% typically                                    │
└─────────────────────────────────────────────────────────────────────────┘
                              ↓

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 4: EXPORT PRUNED CONNECTIONS                                     │
│ Duration: <1 sec                                                        │
│ Code: Lines 1091-1142                                                   │
├─────────────────────────────────────────────────────────────────────────┤
│ Extract which 6 inputs each neuron connects to:                        │
│                                                                         │
│ Example:                                                                │
│   Layer 1, Neuron 0: uses inputs [397, 944, 1154, 1174, 1380, 2568]   │
│   Layer 1, Neuron 1: uses inputs [298, 599, 1168, 2884, 2953, 3179]   │
│   ...                                                                   │
│                                                                         │
│ Output: connections list (saved to pruned_connections_6inputs.pkl)     │
└─────────────────────────────────────────────────────────────────────────┘
                              ↓

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 5: BUILD LUT MODEL WITH FIXED CONNECTIONS                        │
│ Duration: <1 sec                                                        │
│ Code: Lines 1167-1247                                                   │
├─────────────────────────────────────────────────────────────────────────┤
│ Create LUTLayers:                                                       │
│   mapping_layer1 = torch.tensor([[397, 944, ...], ...], dtype=int32)  │
│                                                                         │
│   lut_layer1 = dwn.LUTLayer(                                           │
│       input_size=3200,                                                  │
│       output_size=10,                                                   │
│       n=6,                                                              │
│       mapping=mapping_layer1  ← FIXED connections from pruning         │
│   )                                                                     │
│                                                                         │
│ Model: LUTLayer1 → LUTLayer2                                           │
│                                                                         │
│ Parameters:                                                             │
│   • Connections: FIXED (non-trainable)                                 │
│   • LUT truth tables: Trainable (10 × 64 + 5 × 64 = 960 entries)      │
└─────────────────────────────────────────────────────────────────────────┘
                              ↓

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 6: TRAIN LUT TRUTH TABLES                                        │
│ Duration: ~5-10 min                                                     │
│ Code: Lines 1256-1277                                                   │
├─────────────────────────────────────────────────────────────────────────┤
│ Optimizer: Adam LR=1e-2                                                │
│ Scheduler: CosineAnnealing                                             │
│ Epochs: 50                                                              │
│                                                                         │
│ Training:                                                               │
│   → Only truth table values update                                     │
│   → Connections stay FIXED                                             │
│   → Each LUT learns 64-entry lookup table (2^6)                        │
│                                                                         │
│ Output: Trained LUT model (~70-72% accuracy)                           │
└─────────────────────────────────────────────────────────────────────────┘
                              ↓

┌─────────────────────────────────────────────────────────────────────────┐
│ STAGE 7: SAVE RESULTS                                                  │
│ Code: Lines 1279-1299                                                   │
├─────────────────────────────────────────────────────────────────────────┤
│ Files saved:                                                            │
│   ✓ dwn_lut_model_6inputs.pth          ← Final LUT model              │
│   ✓ pruned_connections_6inputs.pkl     ← Connection mappings          │
│   ✓ pruned_models/jsc_pruned_final.pth ← Pruned linear model          │
│                                                                         │
│ Ready for deployment:                                                   │
│   ✓ FPGA synthesis                                                     │
│   ✓ Hardware simulation                                                │
│   ✓ Further optimization                                               │
└─────────────────────────────────────────────────────────────────────────┘

===============================================================================
                            KEY PARAMETERS
===============================================================================

Configure at lines 1011-1031:

  final_n_weights = 6           ← Change for different LUT sizes (4, 6, 8)
  pruning_steps = 25            ← More = slower but gentler
  epochs_per_step = 6           ← More = better recovery per step
  pruning_steepness = 2.0       ← Lower (1.0) = gentler early pruning
  use_backtracking = True       ← Auto-retry with smaller steps
  backtrack_threshold = 0.05    ← Sensitivity (lower = more sensitive)

===============================================================================
                            EXPECTED TIMELINE
===============================================================================

Fast preset (15 steps):         ~15-20 minutes
Default preset (25 steps):      ~30-40 minutes
Gentle preset (40 steps):       ~60-90 minutes

===============================================================================
                         EXPECTED ACCURACY
===============================================================================

Dense model:                    ~75%
Pruned (6 inputs/neuron):       ~68-70%
After fine-tuning:              ~71-73%
LUT model (truth tables):       ~70-72%

Accuracy drop from dense:       ~3-5%
(Acceptable for 95%+ sparsity and FPGA compatibility!)

===============================================================================
